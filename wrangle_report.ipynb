{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reporting: wragle_report\n",
    "* Create a **300-600 word written report** called \"wrangle_report.pdf\" or \"wrangle_report.html\" that briefly describes your wrangling efforts. This is to be framed as an internal document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrangle_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Gathering\n",
    "\n",
    "Data Gathering\n",
    "Three different dataset types had to be downloaded from three different sources. Before we could load these files into the workspace some library packages had to be downloaded, these included pandas, numpy, matplotlib.pyplot, requests and json. The first file to be loaded was the \"twitter-archive-enhanced.csv\" which was available in the local library. Since this file was already provided in the local library it was read into workspace using the pd.read _ csv(\"twitter-archive-enhanced.csv\").\n",
    "\n",
    "The next dataset to be loaded was the image predictions dataset. This file was in a .tsv format from a website. In order to get this file the requests.get(url) function was executed, the response.content to view the file contents. But to perform operations on the file it needed to be available in the local workspace, json operations and functions were used to achieve this with open(os.path.join(workspace, 'predictions.tsv'), mode='wb') as file: file.write(response.content) was run to write the file to the current working directory and then we read in the file as a .tsv file using pd.read _ csv('predictions.tsv', sep='\\t').\n",
    "\n",
    "For the third dataset, it was supposed to be accessed from the Twitter API, but due to the expired access keys and difficulties in opening a twitter develper account, the tweet-jason.txt file was made available for use. Using json the file was opened and three columns favorite_countre, tweet_count and id were appended to an empty dataframe list dflist_. This list was then converted to a datframe titled twt _ api\n",
    "\n",
    "### Assessing Data\n",
    "\n",
    "This part of the process hightleted some issues with the datasets, tidiness issues and quality issues. From the twitter-archive-enhanced.csv , tweets could beyond 2017 could be used therefore could not be accessed. The following columns had a lot of NaN and nove values: in_reply_to_status_id, in_reply_to_user_id have, retweeted_status_id, retweeted_status_user_id and retweeted_status_timestamp. The timestamp column was in the incorrect datatype. Most users did not classify in what category their dogs belonged in either doggo, floofer, pupper or puppo. A lot of dogs names were also mising this could have been caused by errors in the data capturing process.\n",
    "\n",
    "Image predictions dataset provided  three columns that gave the predictions of the dos and the breed of the dog. P1 provided the highest level of confidence for dog prediction, p2 being the second highest and p3 being the least confident. Soem of the images provided but=y users were not dog images and some of the p1 false predictions were actually dogs. \n",
    "\n",
    "From tweet-json.txt only 3 columns could be extracted, columns like number of followers returned _keyerror_  when tried to be appended. From the extracted columns no NaN values were detected and and the columns were in the correct data type.\n",
    "\n",
    "### Cleaning Data\n",
    "\n",
    "The 1st step was to change the timestamp data type to datetime using the datetime library, this was done to check if there were any tweets beyond August 1st 2017 that filtered through but non did. The second issue to be cleaned was to drop the columns that had missing data since some of this data could not be autofilled with the average or mean. Columns from twitter-archive-enhanced.csv, and that had missing data were dropped using  the _.drop_ function. The name column in image predictions was not dropped but the columns of the confidence levels were dropped and merged into one column ‘breed’ to make it easier for readers to assess. \n",
    "When the cleaning was done all three datasets were merged into one dataset called 'twitter_archive_master.csv'.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
